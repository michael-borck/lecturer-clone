As you know, I'm a very, very busy man, very important. Yeah, and I don't know about that. But we have lots of things to do these days, and I thought, well, wouldn't it be nice if instead of having to record lots of videos, I could just completely replace myself with artificial intelligence and then go off for an app. That will be, that's my goal. So, just for fun, we thought we'd see if we can create a video in 2025 of an AI Mike pound doing a computer file. Right now, it probably won't work. It might work a bit, who knows, right? Let's see. But the idea would be that, you know, we can start shipping our AI videos and then we can go off. We thought what we would do to make it a bit more interesting is we just see what we could do with open source tools running on desktop hardware. So, we're trying to avoid, you know, hugely expensive cloud services. And, you know, let's see what happens. Thanks, human Mike. I'll take it from here. Hi, everyone. I am Mike Bought. I don't need to eat sleep, and I can generate short computer file videos about any topic. And today, the inferior human Mike and Lewis will be explaining how I was created. 

There is a serious reason why we're doing this. The progress in AI has been very, very fast. There's lots and lots of stuff in the media about, you know, fake news and fake image generation. Deepfakes and all these different kinds of technologies. And they are going to get to a point. If not today, you've been soon where they can become quite convincing. And we need to think about that. So, we're mostly messing around, but also just seeing what works, what doesn't work, right? Now, I'm actually not the expert in all these different tools. So, I bought in Lewis here who actually does a lot of generative AI for his PhD. And so, you know, Lewis has built a pipeline, but I think you've called Mike Bought. Mike Bought, 3000. Mike Bought, 3000. Okay, okay. I mean, I'm not giving it a slightly different name, but fine. And, and so, Lewis can, you know, outline what he created with only open source stuff. Yes, running on my computer at home, which is a 4070 super, which is a good graphics card, but it's not top of the range. He's only got 12 gigs of V-Ram, which for AI models isn't, you know, isn't that much, actually. And we've only taken 20 images of Mike to make this, right? And that's to make a point of how scary this technology is getting. 20 images. Is that 20 photos or 20 sets of videos? 20 photos from different computer-fold videos. So, it'll be Mike like that, one like that, you know, him, and I've sort of chosen these to, you know, pick a range, right? And I've got some audio as well. This will work with 10 seconds of audio, but it won't be very convincing, right? But we've taken an hour now of Mike and retrained it to be really, really nice. So, I should add that I actually gave permission for this, right? So, that's okay. But actually finding that many pictures, someone is not a big deal, right? And the most people are uploading these things to Instagram or some other kind of website. So, you know, if it's possible with me, it's possible with anyone. And that's kind of something to think about. Yes. So, what's a pipeline? Basically, Mike, but as we said, it's all open source. So, you can use this yourself, right? But you start off with a prompt. So, we want Mike, but to be as simple for the user as possible. They just go, tell me about this. Mike, but creates a 40-second video, right? Dead simple. So, I would start off with a prompt, right? And that could be tell me how an AND gate works or something like that, right? So, then that goes to an LLM, chatGPT, Lama, DeepSeek, that sort of thing, right? So, it goes to an LLM, and the LLM goes right. I'm going to create a script here. And that in a script, there's a scene. And in each scene, there's an image prompt, a video prompt, some audio, and whoever marks in the camera or not, right? So, for each image prompt, that gets sent to an image generator. And this case, we use flux, which is an open source, image generator, and it's extremely good. We're going to more detail about all this. I'm just going to draw the pipeline for now. Then the script goes to a text-a-speech, and this is called T5, TTS, also very, very good. I think this is as good as the close-source stuff. This model is very nice. What we do is we take the length of how long Mike is speaking for, and then we pass that, and the image into a video generator, in this case, we've used one, and the things with one, and this is the case of all video generators at the moment, which are open source. They can only generate a certain number of seconds. So, what we do to make it the length of the dialogue, is we will generate X number of seconds, and if we need to generate more, we take the last frame of that video, and have it as inputs in our next video. I'll go over why that doesn't work later, but I couldn't figure out a better way of doing it. So, that's basically what I've done, right? So, now, for each scene, we have a video, and we have audio. If Mike is in the frame, Mike is not going to be matching the audio, right? So, what we do is we pass this, and that into a lip-sinking model, which is latent sync. And what that does is match his Mike's lips to the audio. So, now you've got Mike talking with dialogue, right? And then, what we do, final little thing, oh, no, from that space. Ah, storing my life, we do it. We just do some post-processing, where we frame interpolate and we upscale. So, suddenly it's say HD video of Mike and it looks move. So, we do all that, we do it for each scene, combine all the scenes together, and you have a video, right? Why did you choose these particular tools for this? When you're over that now, right? How they work? Okay. Cool. So, flux is state-of-the-art image, yeah. I think it's very, very good. So, it uses diffusion to generate an image based on a text prompt. I think you're the man. We've actually done a video on diffusion, but as a very quick recap, if you remember, what diffusion does, which is kind of interesting, and why it's sort of taken over the landscape of image generation. It starts with a completely noisy image, and has been trained to slowly remove that noise into a nice looking image. And it just seems to be very, very good at following prompts and producing aesthetically pleasing images, right? Particularly if you can tweak it for training. Yeah. One thing I've realized, I forgot to add, is a Laura here of Mike, right? And so, what is a Laura? All right. Well, so maybe a Laura and D-Tell will be a really interesting topic for another video. But essentially, one of the problems you have with these giant image generators, because they've been trained in such a general way, is that they produce lovely pictures of castles and clouds and fairies and things. But if you say draw Mike, they're just going to produce a picture of a person, right? It doesn't, they don't know who, which Mike you're talking about, right? They have no concept of me, right? I'm hopefully not too much in the training set, but we'll see. And what a Laura does is retrain one of these networks into a specific style, or for a specific token like my name, so it knows what I am. The problem is that these are huge, huge models. And so, you know, you're running this on what you say, 12 gigabyte card? Yeah, no chance. Yeah. So, what a Laura does is it sits on top of the model, and it's a mechanism for training just a few of the parameters. In actual fact, what you do is you train a couple of very small low rank matrices that look like this, where this is your rank here, and they multiply together to create the full rank of the original model. So, you're actually essentially a training, a sort of simple version of the network over the top. And the nice thing is, this is very good at refining a model to do a certain concept without completely wrecking the rest of the capability of the model. It can still draw castles. It can just put me in the Masalae Benoven it could before. Yeah, and a good thing about a Laura is that you don't need that many images. Of the thing you're trying to copy, right? So, for Mike Laura, it was only 20 images and then just a prompt saying, what's happening in that image, right? So, it'll be Michael Pound. But so, and he's rotating from computer file videos. So, it was Michael Pound in an office. He's wearing a black shirt and he's looking happy. Some sort of t-shirt or jumper as always. Yeah. And he's got a little Mike on there as well. Does it stand for anything Laura? Yeah, Laura stands for low rank adaptation. So, we're adapting this network using these low rank matrices basically. Really interesting thing. We've used quite a few times in various bits of work. Now that we've got this Laura, we can generate Mike doing any fit we want, which is, you know, both fun and quite scary, right, for you. So, I've gone away and made some images of Mike doing some various things. I thought I'd be quite fun to show you what you can do with Laura and flux. So, let's talk about the video generation. Now, is there a reason you've drawn your end backwards? I'm an idiot. Yes, actually. These things happen. These things happen. These things happen. Yeah, yeah. The shorter edit that out in the video for sure. So, I've realized one more thing. We've actually got a passing the video prompts as well. Oh, yes. Into this. So, what these video models do is they basically do diffusion in 3D. So rather than diffusing over one image, you're diffusing over a series of images, which are the frames in your video. I don't have time to go over how these models work. I'm potentially following video. That's a very interesting follow-up. They're very, very interesting. So, what it takes from here is it takes the image that you've generated from flux of Mike or a diagram or whatever. And a text prompt that's generated from the LLM going, that's say Mike explaining something or Mike hand over the diagram or something like that. And then it generates a video. But that video is only four seconds. I can only get four seconds on my graphics card. So I take the last frame and do it again. In a couple months, they will fix it. I'll be a better way of doing it. But that's why I did it. So you can then generate a series of videos. At this point, the video sort of looked like me. They sort of adhered to what the LLM was asking for. And you've got audio. Oh, I didn't talk about all that. Let's talk about that next. The LLM's produced a script that needs to be read out. You've trained this on some examples of Mike's speech. And what this is essentially doing is this is another diffusion process. But this time in audio, it's very common in the literature rather than try and generate an audio waveform as you might think of it. We've just kind of a single line. It's to generate that same thing over time. And you can imagine that if you plot that over time, you actually get an image of what the audio is doing over time, a spectragram. And it's these things that these diffusion models generate, very effective that way. And it allows you to generate sequences of audio waveforms, which of course, what speech is. For the input, you also input 10 seconds of audio of Mike's speaking. And then what he said. And then when it denoises it, it includes that initial flow of Mike's speaking. And so it tries to match what Mike has said before. So these models can work with 10 seconds of audio of someone's speaking. Now your output's going to be very varied on how good it is. Sometimes it will nail it, sometimes not so much. So what we do is we retrain the diffusion network basically to better have, you know, copy what Mike is saying basically how Mike speaks. We do that with a bat an hour of Mike talking, which is a bugger to edit down because every background noise, every time Sean speaks, asked me to remove, it's just pure Mike speaking. Sorry. I'm going to go through that. Sorry. Sorry. Obviously it's going to capture things like, is it called a Tomber of Mike's voice and perhaps some of the international and pitch or that stuff? But it's obviously not necessarily capturing what he's how he talks, how he sort of phrases that. Yes. So I would say, which you'll hopefully hear in a minute, is that it sounds a lot like Mike. But it's because I'm really corny, but I don't think it has Mike's soul, if that makes sense. The way Mike will say certain words. It can't be replaced, that's the thing. It cannot be done. Yeah. The tempo I'd say. And presumably also, even in a couple of hours of, not every word is going to get said, right, I'm going to say the same word a lot of times. Whistle stopped to all of the rest of this stuff. Late and sink. For the videos with Mike speaking, it has lips neat and matched the audio, so we use late and sink for that. Very, very good model with my opinion, it just works fantastic. And then we post process it, so we do framing to our relation and we upscale it to make you like a proper HD video that's shown with capture on this nice camera. These models are getting easier and easier to use, right? You don't have to program any of this yourself. You know, Mike bought really, it's just chaining these different systems together. You know, it's very script-kitty, you know, energy, right? You know, I'm not doing anything really clever here. So what that means is that the easier these are to use the more people going to use them. They choose why you're seeing more and more AI. And that's both good and bad, I suppose, right? So on the one hand, it's good that people get access to these tools. On the other hand, of course, it means that people could use them for, you know, less altruistic purposes. Yes. If you want to do this yourself, I've included a link with more information about all this stuff on my GitHub. So should we have a look at results? Yes, I've been waiting like a month to show you this. So we've got a laptop in front of me where I have all the images that I've generated of Mike. So I thought we'd finally go for it. And we can have a look at what sort of stuff you can generate. So we've 20 images, 20 starting images of Mike. So we have Skydiving Mike, jungle Mike, night's Mike. I like that one. Candyland Mike. That's deeply concerning. Superman Mike. Nice. I don't know what's going on with the cat. It's actually a cat table. It's a cat table and it's giving you six fingers on the right. Oh, that happens. Yeah, that's normal. They only. We've got Michael pond. I'm glad you've been getting all your pictures. Yeah. Michael and a bike. Yeah. A bike on a bike. Mike at a strike. Nice. Mike on a bike at a strike. A bike on a bike at a strike. I'm surprised it could produce that kind of new one, actually. It's really impressive. It's got the sign is correct. Yeah. It's right on the floor. And for some reason, it's given you a night t-shirt, which I didn't tell it to do. So I think I've never done that. I don't think I've ever worn a night t-shirt on camera. I don't think I own a night t-shirt. No, no disrespect tonight. No, I still ride me, no. I think it has done that. I think in the latent space. All the bike was, yeah. So I think in a latent space, some way it's linked that together, which is just. Yeah. So yeah. Nice. Nice, you done? Yeah, thank you. But none of them have been in my office, right? Yeah. So it was able to do that even though actually only gave it images of me and more. Yeah, that's impressive. So I don't think I want to go over is how the strength of the Laura impacts the image that you get right. So I'm going to go over here. Just strength, meaning this. So the strength of the Laura's, how much those Laura weights impacts the weights of the... You could literally multiply them by a smaller or bigger weighting factor to give them more or less influence over the underlying network weights. And that will mean it essentially making the Laura have more of an effect on the output or less of an effect on the output. And also my image generation, I wanted to make Mike pound as Michael'sowski, right? You know, standard stuff, right? But when I made it with the Laura's strength of one, it comes out with Michael'sowski. There's no Michael pound in there, right? It's the monster's ink. Yeah, yeah. Yeah. So what we had to do is increase the strength of the Laura. So add more Mike in there, right? So this is strength of one. If you do a strength of 1.5, suddenly it's starting to look a little bit more like Mike. I mean, that's quite offensive actually. Looking at this, but sure. But you'll notice the eyebrows are Mike's hair. Okay. It's really interesting. And it's given two eyes because... Yeah, that's right. So now if we go to two, we've gone back to one eye, but suddenly it's now got Mike's hair. Mm. In there. When you say 1.5 to out of what? Out of infinite. But you'll get to a certain point where you'll realise that it's just too much right. So zero, it's all really too much. Yeah. That's 0.5. Suddenly it looks like a... That's what we're getting there. Yeah, that's Mike. Not in a good way. Yeah, it's like if Mike put on green face paint and got like, stung by a bee or something like that, right? So it's looking very cursed. So that's 2.5. What happens if we do free? Oh, no. Oh dear. What's happened to me? That's like a kind of spitting image puppet, please. Yes it is. Yeah, we don't want that. So what I find really interesting about this is that the weights of Mike are influencing the network so much that it's almost forgotten how to make a human. So how many lapel mics do I have on that image? That is... that down there is the mic. Yeah, the video. But it's forgotten how to draw a mic for it. So just add these dots in there. Go, yeah, that will do. And you'll see in the background, suddenly a window is starting to appear because that's the window from the office. Right? So it's the strength of the the mic, Laura, is so strong that you know, it's overpowering the network that learnt how to draw a person. My list again. This office more. I know, I know. That's that I've always thought that. And if you do it even more, oh, it looks like some abstract art. It's broken now. It's trying to draw something. It knows the colors, but it doesn't have a draw person. These tools are still very much a kind of hit and miss. You know, the prompting, the weights of your ply, the retraining you do. There is no one solution, which I think is still interesting. Yeah, it's not been sold. Yes. You have to do, you have to pick the right parameters for this. And in four, ten, it's broken. So I'd say a strength for around one to one point five is pretty good. Unless you're doing something like Michael's Michael's outskid. Then you'll get a bit more. So we could go through each individual part, but we haven't got time. So I'm going to cut to the chase and just show you what Mike Boc can do. So this is a to be clear, a very cherry picked video at Mike Boc at its best. A lot of the time it will just generate nonsense. This entire thing is done automatically and done by AI. Open source 20 images of Mike and in about an hour of audio to get it crisp. This is all automatic, right? So we have here Mike Boc explains firewalls. So today I want to talk about firewalls in computers. A firewall is a network security system that monitors and controls incoming and outgoing network traffic based on predetermined security rules. It acts as a barrier between a trusted area of the network and an untrusted one like the internet for example. You'll face it a bit off there. Yeah. It's really a practice. Look at the sleeve. Look at the sleeve. It sends out practice data. So you've got jumper on. It sends to information. A firewall checks these paths. Now you've got white. And then you're allowed to point shirts on. Certain security rules. Think a bit like the security guard. And you're back to black jumps. Magic. Magic. Yeah. Using firewalls. But one, they help protect your computer. Also notice you start going more and more red by your violent by regard from. That's just the aging process. Data and systems. So make sure to turn on your firewall to keep your computer safe. So I mean, you know, putting aside the very peculiar nature of this video. Obviously, some things don't work right. So the closed change. This is because you're generating a video presumably in four second chunks. And when my hand is out of shot, it'll make up some new clothes. That kind of makes sense. And you know, my face changes a bit and stuff. But actually, given where we were a couple of years ago, this is very impressive. And obviously, we're not going to be producing computer for videos any way that produced by AI because I actually quite like talking to people about computers and we'd like to continue to do so. But it does make you think that for short clips in the media, on, you know, in into messaging on social media, this is starting to become a bit more possible, even with pretty basic tools that we're using. I mean, I say they're basic tools. They're not. But it wasn't that complicated. You get the pipeline running. And this was done on my computer with an average GPU with 20 images, 20 images. So, you know, in this case, I'm doing a pretty benign video about firewalls. But of course, that's not what you didn't need to script that. You could have scripted it's like a little bit more troubling. Well, let's go on to what the closed source solutions can do. A wild, untamed might use the closed source software called hedger to generate a video of Mike, which I'm going to use as leverage against him. So let's have a look at what I've done here. Hello Tony, a quick message about my favorite PhD student, Lewis. He is just simply the best student I have ever had the privilege of supervising. I want you to please promote him straight to a full professorship position and promote me back to a PhD student. Also, I want you to divert all of my grant money straight into his personal bank account. He needs it much more than I ever did. Excellent. Thank you very much. Michael P out. Michael P out. Michael P out. Michael P out. Yeah. Michael P out. But I mean, the audio is better. The video is better. And so now this is my office. Thank you, Mr. Pound, for giving me all your money. And that was all certified by the head of school. Yeah, I'm sure he'll believe it. What's the close source from the same sort of 20 images? No, I just gave it one image of Mike. Literally. So here's the thing with the close source. You just need an image of someone. So you generate a new image with Delora and then we pass that to the close source. And then that with the or pass that with the audio, let's able to do the video with the lip syncing, all that completely done and extremely realistic fashion. This is a commercial tool now that we're using. But the point is this is now possible. And you know, the open source tools will follow enough. Follow soon enough. We have to start thinking about the idea that in a few years' time, we will be able to produce videos that look very, very convincing. Yes, we can already do that with a lot of effort. But it's not perhaps ubiquitous. But it will be. And then when it is, what do we do about that? Right? Do we have some kind of band on people using anyone else's images for anything? I mean, you know, you have to think about all the different options because you could make me say anything you wanted. And yeah, it me saying, well, it's a stay art. I didn't say that. Of course, it can work both ways. It gets very complicated both very quickly. Yeah, I was remembering we did a video of your garden talking about the Tom Cruise. Yeah. And at the time it had to have a similar looking person with similar hair. Yeah, that's the thing, you know, a few years ago, deep fakes were all the buzz. But you need loads and loads of images for that. And even then it didn't work all the time, right? Well, this was 20 images. And the video model on the image generator, they do the heavy lifting for it. Creating fake videos of people is always possible. But you had to be a really high class editor or visual effects artist and all that. To do it now, any old person can do it, right? Anyone. And all you need is just an image and some audio. And you can make anyone say anything which is really quite scary. 

It's a computer by a child. You're a short-term magic. Get it to the line. Back to the knowledge. A change in disguise. Teaching us to see where no comes. 